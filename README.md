# Active Flow Control of a 2D Cylinder using Deep Reinforcement Learning

This repository contains code for implementing a deep reinforcement learning algorithm to control the flow around a 2D circular cylinder. The objective is to minimize the drag force acting on the cylinder by manipulating the flow using an optimal control policy learned through reinforcement learning.

## Background

The study of flow around a cylindrical body is one of the classical fluid mechanics and flow control problems. Active flow control is a new and exciting area that has the potential to revolutionize industries that deal with wind turbines and aerospace vehicles. This project aims to use deep reinforcement learning to learn an optimal control policy that can manipulate the flow around a 2D circular cylinder, based on the approach described in the paper "Active flow control of a 2D cylinder using deep reinforcement learning" by Zhu et al. (2018).

## Getting Started

### Prerequisites

This project has the following dependencies:

- TensorFlow 1.8.0
- TensorForce 0.4.2
- Gmsh (version 3.+ but not 4.+)
- FEniCS (version 2017.1.0 or 2017.2.0)

### Data

The data for this project will be generated using computational fluid dynamics (CFD) simulations. The simulations will be performed using the Finite Element Method (FEM) and the FEniCS software package. The FEniCS simulations will generate the flow field around the cylinder for each control input. The control inputs will be manipulated to achieve the desired flow control, and the corresponding drag force acting on the cylinder will be recorded.

### Method and Algorithm

The code uses deep reinforcement learning to learn an optimal control policy that can manipulate the flow around a 2D circular cylinder to minimize the drag force acting on the cylinder.

The algorithm used in the original code is Proximal Policy Optimization (PPO), a reinforcement learning algorithm that uses a policy gradient approach to update the agent's policy. To fit the requirements of the flow control problem, the PPO algorithm is modified.

To generate the mesh for the 2D cylinder geometry, the Gmsh software package is used. The FEniCS software package is used to solve the Navier-Stokes equations for the fluid flow. The OpenAI Gym interface defines the environment for the reinforcement learning agent, while TensorFlow is used to implement the neural network for the policy and value functions.

### Credit

This code is based on the work of several researchers who have made significant contributions to the field of flow control using deep reinforcement learning. Specifically, this code is inspired by the following sources:

- Zhu et al. (2018). "Active flow control of a 2D cylinder using deep reinforcement learning". ArXiv:1808.07664 [cs.LG].
- Jerabaul29 (https://github.com/jerabaul29/Cylinder2DFlowControlDRL)
- Darshan Patel (https://github.com/darshan315/flow_past_cylinder_by_DRL).
- Shahzeb Aamir (https://github.com/ShahzebAamir/DRL-for-Active-Flow-Control).
- Fabian Gabriel (https://github.com/FabianGabriel/Active_flow_control_past_cylinder_using_DRL). 
- The paper "Active flow control of a circular cylinder using deep reinforcement learning" by Nakata et al. (2020) (https://doi.org/10.1063/1.5136859).
- The paper "Active flow control of a circular cylinder wake using deep reinforcement learning" by Shotorban et al. (2020)   (https://doi.org/10.1016/j.jfluidstructs.2020.103107).

If you use this code or any part of it in your research, please provide appropriate credit to the sources mentioned above and cite their original work.

### Evaluation and Interpretation of Model

The model will be evaluated based on its ability to minimize the drag force acting on the 2D circular cylinder. The primary metric used for evaluation will be the minimum drag coefficient achieved by the model.

To evaluate the performance of the model, the trained agent will be used to generate animations of the flow field around the cylinder for both cases with flow control and without flow control. The results obtained will be compared with other flow control methods used previously to determine the effectiveness of the deep reinforcement learning method in comparison to classical methods.

Additionally, the behavior of the trained agent's control policy will be analyzed to provide insight into the learned flow control mechanism. This will involve visualizing the changes in the flow field resulting from the control inputs generated by the agent, as well as analyzing the learned policy's sensitivity to variations in the flow conditions.

Overall, the evaluation and interpretation of the model will aim to provide insights into the effectiveness of deep reinforcement learning for flow control and shed light on the underlying flow control mechanism learned by the agent.
